[{"title": "Steven Pinker: AI in the Age of Reason | Lex Fridman Podcast #3", "id": "epQxfSp-rdU", "quotes": [{"time": 412, "text": "That is, we're neural networks, natural selection did a kind of equivalent of engineering of our brains."}, {"time": 419, "text": "So I don't think there's anything mysterious in the sense that no system made out of silicon could ever do what a human brain can do."}, {"time": 424, "text": "I think it's possible in principle."}, {"time": 431, "text": "Whether it'll ever happen depends not only on how clever we are in engineering these systems, but whether we even want to, whether that's even a sensible goal."}, {"time": 442, "text": "That is, you can ask the question, is there any locomotion system that is as good as a human?"}, {"time": 449, "text": "Well, we kind of want to do better than a human ultimately in terms of legged locomotion."}, {"time": 455, "text": "There's no reason that humans should be our benchmark."}, {"time": 455, "text": "They're tools that might be better in some ways."}, {"time": 459, "text": "It may be that we can't duplicate a natural system because at some point it's so much cheaper to use a natural system that we're not going to invest more brainpower and resources."}, {"time": 474, "text": "So for example, we don't really have an exact substitute for wood."}, {"time": 474, "text": "We still build houses out of wood."}, {"time": 480, "text": "We still build furniture out of wood."}, {"time": 480, "text": "We like the look."}, {"time": 480, "text": "We like the feel."}, {"time": 480, "text": "It has certain properties that synthetics don't."}, {"time": 485, "text": "It's not that there's anything magical or mysterious about wood."}, {"time": 491, "text": "It's just that the extra steps of duplicating everything about wood is something we just haven't bothered because we have wood."}, {"time": 497, "text": "Likewise, say cotton."}, {"time": 497, "text": "I'm wearing cotton clothing now."}, {"time": 497, "text": "It feels much better than polyester."}, {"time": 501, "text": "It's not that cotton has something magic in it."}, {"time": 501, "text": "It's not that we couldn't ever synthesize something exactly like cotton, but at some point it's just not worth it."}, {"time": 509, "text": "We've got cotton."}, {"time": 515, "text": "Likewise, in the case of human intelligence, the goal of making an artificial system that is exactly like the human brain is a goal that we probably know is going to pursue to the bitter end, I suspect, because if you want tools that do things better than humans, you're not going to care whether it does something like humans."}, {"time": 533, "text": "So for example, diagnosing cancer or predicting the weather, why set humans as your benchmark?"}, {"time": 538, "text": "But in general, I suspect you also believe that even if the human should not be a benchmark and we don't want to imitate humans in their system, there's a lot to be learned about how to create an artificial intelligence system by studying the human."}, {"time": 555, "text": "Yeah, I think that's right."}, {"time": 555, "text": "In the same way that to build flying machines, we want to understand the laws of aerodynamics, including birds, but not mimic the birds, but they're the same laws."}, {"time": 567, "text": "You have a view on AI, artificial intelligence, and safety that, from my perspective, is refreshingly rational or perhaps more importantly, has elements of positivity to it, which I think can be inspiring and empowering as opposed to paralyzing."}, {"time": 593, "text": "For many people, including AI researchers, the eventual existential threat of AI is obvious, not only possible, but obvious."}, {"time": 599, "text": "And for many others, including AI researchers, the threat is not obvious."}, {"time": 605, "text": "So Elon Musk is famously in the highly concerned about AI camp, saying things like AI is far more dangerous than nuclear weapons, and that AI will likely destroy human civilization."}, {"time": 621, "text": "Human civilization."}, {"time": 621, "text": "So in February, he said that if Elon was really serious about AI, the threat of AI, he would stop building self driving cars that he's doing very successfully as part of Tesla."}, {"time": 635, "text": "Then he said, wow, if even Pinker doesn't understand the difference between narrow AI, like a car and general AI, when the latter literally has a million times more compute power and an open ended utility function, humanity is in deep trouble."}, {"time": 647, "text": "So first, what did you mean by the statement about Elon Musk should stop building self driving cars if he's deeply concerned?"}, {"time": 660, "text": "Not the last time that Elon Musk has fired off an intemperate tweet."}, {"time": 664, "text": "Well, we live in a world where Twitter has power."}, {"time": 667, "text": "Yeah, I think there are two kinds of existential threat that have been discussed in connection with artificial intelligence, and I think that they're both incoherent."}, {"time": 680, "text": "One of them is a vague fear of AI takeover, that just as we subjugated animals and less technologically advanced peoples, so if we build something that's more advanced than us, it will inevitably turn us into pets or slaves or domesticated animal equivalents."}, {"time": 728, "text": "There's no reason to think that sheer problem solving capability will set that as one of its goals."}, {"time": 733, "text": "Its goals will be whatever we set its goals as, and as long as someone isn't building a megalomaniacal artificial intelligence, then there's no reason to think that it would naturally evolve in that direction."}, {"time": 744, "text": "Now, you might say, well, what if we gave it the goal of maximizing its own power source?"}, {"time": 748, "text": "That's a pretty stupid goal to give an autonomous system."}, {"time": 748, "text": "You don't give it that goal."}, {"time": 754, "text": "I mean, that's just self evidently idiotic."}, {"time": 754, "text": "So if you look at the history of the world, there's been a lot of opportunities where engineers could instill in a system destructive power and they choose not to because that's the natural process of engineering."}, {"time": 769, "text": "Well, except for weapons."}, {"time": 769, "text": "I mean, if you're building a weapon, its goal is to destroy people, and so I think there are good reasons to not build certain kinds of weapons."}, {"time": 773, "text": "I think building nuclear weapons was a massive mistake."}, {"time": 778, "text": "So maybe pause on that because that is one of the serious threats."}, {"time": 786, "text": "Do you think that it was a mistake in a sense that it should have been stopped early on?"}, {"time": 792, "text": "Or do you think it's just an unfortunate event of invention that this was invented?"}, {"time": 799, "text": "Do you think it's possible to stop?"}, {"time": 799, "text": "I guess is the question."}, {"time": 799, "text": "It's hard to rewind the clock because of course it was invented in the context of World War II and the fear that the Nazis might develop one first."}, {"time": 808, "text": "Then once it was initiated for that reason, it was hard to turn off, especially since winning the war against the Japanese and the Nazis was such an overwhelming goal of every responsible person that there's just nothing that people wouldn't have done then to ensure victory."}, {"time": 827, "text": "It's quite possible if World War II hadn't happened that nuclear weapons wouldn't have been invented."}, {"time": 861, "text": "I think analogies between nuclear weapons and artificial intelligence are fundamentally misguided because the whole point of nuclear weapons is to destroy things."}, {"time": 865, "text": "The point of artificial intelligence is not to destroy things."}, {"time": 870, "text": "So the analogy is misleading."}, {"time": 876, "text": "So there's two artificial intelligence you mentioned."}, {"time": 876, "text": "The first one I guess is highly intelligent or power hungry."}, {"time": 882, "text": "Yeah, it's a system that we design ourselves where we give it the goals."}, {"time": 882, "text": "Goals are external to the means to attain the goals."}, {"time": 886, "text": "If we don't design an artificially intelligent system to maximize dominance, then it won't maximize dominance."}, {"time": 895, "text": "It's just that we're so familiar with homo sapiens where these two traits come bundled together, particularly in men, that we are apt to confuse high intelligence with a will to power, but that's just an error."}, {"time": 915, "text": "The other fear is that will be collateral damage that will give artificial intelligence a goal like make paper clips and it will pursue that goal so brilliantly that before we can stop it, it turns us into paper clips."}, {"time": 927, "text": "We'll give it the goal of curing cancer and it will turn us into guinea pigs for lethal experiments or give it the goal of world peace and its conception of world peace is no people, therefore no fighting and so it will kill us all."}, {"time": 938, "text": "Now I think these are utterly fanciful."}, {"time": 943, "text": "In fact, I think they're actually self defeating."}, {"time": 972, "text": "I think that the collateral damage scenario, the value alignment problem is also based on a misconception."}, {"time": 978, "text": "So one of the challenges, of course, we don't know how to build either system currently or are we even close to knowing?"}, {"time": 983, "text": "Of course, those things can change overnight, but at this time, theorizing about it is very challenging in either direction."}, {"time": 987, "text": "So that's probably at the core of the problem is without that ability to reason about the real engineering things here at hand is your imagination runs away with things."}, {"time": 999, "text": "But let me sort of ask, what do you think was the motivation, the thought process of Elon Musk?"}, {"time": 1005, "text": "I build autonomous vehicles, I study autonomous vehicles, I study Tesla autopilot."}, {"time": 1012, "text": "I think it is one of the greatest currently large scale application of artificial intelligence in the world."}, {"time": 1017, "text": "It has potentially a very positive impact on society."}, {"time": 1024, "text": "So how does a person who's creating this very good quote unquote narrow AI system also seem to be so concerned about this other general AI?"}, {"time": 1030, "text": "What do you think is the motivation there?"}, {"time": 1039, "text": "What do you think is the thing?"}, {"time": 1039, "text": "Well, you probably have to ask him, but there, and he is notoriously flamboyant, impulsive to the, as we have just seen, to the detriment of his own goals of the health of the company."}, {"time": 1051, "text": "So I don't know what's going on in his mind."}, {"time": 1057, "text": "You probably have to ask him, but I don't think the, and I don't think the distinction between special purpose AI and so called general AI is relevant that in the same way that special purpose AI is not going to do anything conceivable in order to attain a goal."}, {"time": 1070, "text": "All engineering systems are designed to trade off across multiple goals."}, {"time": 1077, "text": "When we build cars in the first place, we didn't forget to install brakes because the goal of a car is to go fast."}, {"time": 1082, "text": "It occurred to people, yes, you want it to go fast, but not always."}, {"time": 1088, "text": "So you would build in brakes too."}, {"time": 1088, "text": "Likewise, if a car is going to be autonomous and program it to take the shortest route to the airport, it's not going to take the diagonal and mow down people and trees and fences because that's the shortest route."}, {"time": 1104, "text": "That's not what we mean by the shortest route when we program it."}, {"time": 1104, "text": "And that's just what an intelligence system is by definition."}, {"time": 1109, "text": "It takes into account multiple constraints."}, {"time": 1116, "text": "The same is true, in fact, even more true of so called general intelligence."}, {"time": 1116, "text": "That is, if it's genuinely intelligent, it's not going to pursue some goal singlemindedly, omitting every other consideration and collateral effect."}, {"time": 1128, "text": "That's not artificial and general intelligence."}, {"time": 1128, "text": "That's artificial stupidity."}, {"time": 1134, "text": "I agree with you, by the way, on the promise of autonomous vehicles for improving human welfare."}, {"time": 1141, "text": "I think it's spectacular."}, {"time": 1141, "text": "And I'm surprised at how little press coverage notes that in the United States alone, something like 40,000 people die every year on the highways, vastly more than are killed by terrorists."}, {"time": 1151, "text": "And we spent a trillion dollars on a war to combat deaths by terrorism, about half a dozen a year."}, {"time": 1158, "text": "Whereas year in, year out, 40,000 people are massacred on the highways, which could be brought down to very close to zero."}, {"time": 1164, "text": "So I'm with you on the humanitarian benefit."}, {"time": 1169, "text": "Let me just mention that as a person who's building these cars, it is a little bit offensive to me to say that engineers would be clueless enough not to engineer safety into systems."}, {"time": 1179, "text": "I often stay up at night thinking about those 40,000 people that are dying."}, {"time": 1185, "text": "And everything I tried to engineer is to save those people's lives."}, {"time": 1185, "text": "So every new invention that I'm super excited about, in all the deep learning literature and CVPR conferences and NIPS, everything I'm super excited about is all grounded in making it safe and help people."}, {"time": 1199, "text": "So I just don't see how that trajectory can all of a sudden slip into a situation where intelligence will be highly negative."}, {"time": 1213, "text": "You and I certainly agree on that."}, {"time": 1213, "text": "And I think that's only the beginning of the potential humanitarian benefits of artificial intelligence."}, {"time": 1217, "text": "There's been enormous attention to what are we going to do with the people whose jobs are made obsolete by artificial intelligence, but very little attention given to the fact that the jobs that are going to be made obsolete are horrible jobs."}, {"time": 1232, "text": "The fact that people aren't going to be picking crops and making beds and driving trucks and mining coal, these are soul deadening jobs."}, {"time": 1238, "text": "And we have a whole literature sympathizing with the people stuck in these menial, mind deadening, dangerous jobs."}, {"time": 1245, "text": "If we can eliminate them, this is a fantastic boon to humanity."}, {"time": 1253, "text": "Now granted, you solve one problem and there's another one, namely, how do we get these people a decent income?"}, {"time": 1258, "text": "But if we're smart enough to invent machines that can make beds and put away dishes and handle hospital patients, I think we're smart enough to figure out how to redistribute income to apportion some of the vast economic savings to the human beings who will no longer be needed to make beds."}, {"time": 1279, "text": "Sam Harris says that it's obvious that eventually AI will be an existential risk."}, {"time": 1286, "text": "He's one of the people who says it's obvious."}, {"time": 1291, "text": "We don't know when the claim goes, but eventually it's obvious."}, {"time": 1291, "text": "And because we don't know when, we should worry about it now."}, {"time": 1298, "text": "This is a very interesting argument in my eyes."}, {"time": 1298, "text": "So how do we think about timescale?"}, {"time": 1305, "text": "How do we think about existential threats when we don't really, we know so little about the threat, unlike nuclear weapons perhaps, about this particular threat, that it could happen tomorrow, right?"}, {"time": 1318, "text": "So, but very likely it won't."}, {"time": 1318, "text": "Very likely it'd be a hundred years away."}, {"time": 1324, "text": "So how do we ignore it?"}, {"time": 1324, "text": "How do we talk about it?"}, {"time": 1324, "text": "Do we worry about it?"}, {"time": 1324, "text": "How do we think about those?"}, {"time": 1332, "text": "What is it?"}, {"time": 1333, "text": "A threat that we can imagine."}, {"time": 1333, "text": "It's within the limits of our imagination, but not within our limits of understanding to accurately predict it."}, {"time": 1344, "text": "But what is the it that we're afraid of?"}, {"time": 1346, "text": "AI being the existential threat."}, {"time": 1350, "text": "Like enslaving us or turning us into paperclips?"}, {"time": 1355, "text": "I think the most compelling from the Sam Harris perspective would be the paperclip situation."}, {"time": 1359, "text": "I mean, I just think it's totally fanciful."}, {"time": 1359, "text": "I mean, that is don't build a system."}, {"time": 1363, "text": "Don't give a, don't, first of all, the code of engineering is you don't implement a system with massive control before testing it."}, {"time": 1370, "text": "Now, perhaps the culture of engineering will radically change."}, {"time": 1375, "text": "Then I would worry, but I don't see any signs that engineers will suddenly do idiotic things, like put a electric power plant in control of a system that they haven't tested first."}, {"time": 1387, "text": "Or all of these scenarios, not only imagine almost a magically powered intelligence, including things like cure cancer, which is probably an incoherent goal because there's so many different kinds of cancer or bring about world peace."}, {"time": 1400, "text": "I mean, how do you even specify that as a goal?"}, {"time": 1405, "text": "But the scenarios also imagine some degree of control of every molecule in the universe, which not only is itself unlikely, but we would not start to connect these systems to infrastructure without testing as we would any kind of engineering system."}, {"time": 1425, "text": "Now, maybe some engineers will be irresponsible and we need legal and regulatory and legal responsibility implemented so that engineers don't do things that are stupid by their own standards."}, {"time": 1440, "text": "But the, I've never seen enough of a plausible scenario of existential threat to devote large amounts of brain power to, to forestall it."}, {"time": 1448, "text": "So you believe in the sort of the power on mass of the engineering of reason, as you argue in your latest book of Reason and Science, to sort of be the very thing that guides the development of new technology so it's safe and also keeps us safe."}, {"time": 1468, "text": "You know, granted the same culture of safety that currently is part of the engineering mindset for airplanes, for example."}, {"time": 1474, "text": "So yeah, I don't think that that should be thrown out the window and that untested all powerful systems should be suddenly implemented, but there's no reason to think they are."}, {"time": 1485, "text": "And in fact, if you look at the progress of artificial intelligence, it's been, you know, it's been impressive, especially in the last 10 years or so, but the idea that suddenly there'll be a step function that all of a sudden before we know it, it will be all powerful, that there'll be some kind of recursive self improvement, some kind of fume is also fanciful."}, {"time": 1506, "text": "We, certainly by the technology that we, that we're now impresses us, such as deep learning, where you train something on hundreds of thousands or millions of examples, they're not hundreds of thousands of problems of which curing cancer is a typical example."}, {"time": 1526, "text": "And so the kind of techniques that have allowed AI to increase in the last five years are not the kind that are going to lead to this fantasy of exponential sudden self improvement."}, {"time": 1531, "text": "I think it's kind of a magical thinking."}, {"time": 1540, "text": "It's not based on our understanding of how AI actually works."}, {"time": 1545, "text": "Now give me a chance here."}, {"time": 1545, "text": "So you said fanciful, magical thinking."}, {"time": 1545, "text": "In his TED talk, Sam Harris says that thinking about AI killing all human civilization is somehow fun, intellectually."}, {"time": 1555, "text": "Now I have to say as a scientist engineer, I don't find it fun, but when I'm having beer with my non AI friends, there is indeed something fun and appealing about it."}, {"time": 1568, "text": "Like talking about an episode of Black Mirror, considering if a large meteor is headed towards Earth, we were just told a large meteor is headed towards Earth, something like this."}, {"time": 1574, "text": "And can you relate to this sense of fun?"}, {"time": 1580, "text": "And do you understand the psychology of it?"}, {"time": 1584, "text": "I personally don't find it fun."}, {"time": 1584, "text": "I find it kind of actually a waste of time because there are genuine threats that we ought to be thinking about like pandemics, like cyber security vulnerabilities, like the possibility of nuclear war and certainly climate change."}, {"time": 1606, "text": "You know, this is enough to fill many conversations."}, {"time": 1606, "text": "And I think Sam did put his finger on something, namely that there is a community, sometimes called the rationality community, that delights in using its brainpower to come up with scenarios that would not occur to mere mortals, to less cerebral people."}, {"time": 1627, "text": "So there is a kind of intellectual thrill in finding new things to worry about that no one has worried about yet."}, {"time": 1634, "text": "I actually think, though, that it's not only is it a kind of fun that doesn't give me particular pleasure, but I think there can be a pernicious side to it, namely that you overcome people with such dread, such fatalism, that there are so many ways to die, to annihilate our civilization, that we may as well enjoy life while we can."}, {"time": 1659, "text": "There's nothing we can do about it."}, {"time": 1659, "text": "If climate change doesn't do us in, then runaway robots will."}, {"time": 1662, "text": "So let's enjoy ourselves now."}, {"time": 1662, "text": "We've got to prioritize."}, {"time": 1662, "text": "We have to look at threats that are close to certainty, such as climate change, and distinguish those from ones that are merely imaginable but with infinitesimal probabilities."}, {"time": 1678, "text": "And we have to take into account people's worry budget."}, {"time": 1685, "text": "You can't worry about everything."}, {"time": 1685, "text": "And if you sow dread and fear and terror and fatalism, it can lead to a kind of numbness."}, {"time": 1692, "text": "Well, these problems are overwhelming, and the engineers are just going to kill us all."}, {"time": 1697, "text": "So let's either destroy the entire infrastructure of science, technology, or let's just enjoy life while we can."}, {"time": 1706, "text": "So there's a certain line of worry, which I'm worried about a lot of things in engineering."}, {"time": 1712, "text": "There's a certain line of worry when you cross, you're allowed to cross, that it becomes paralyzing fear as opposed to productive fear."}, {"time": 1716, "text": "And that's kind of what you're highlighting."}, {"time": 1724, "text": "And we've seen some, we know that human effort is not well calibrated against risk in that because a basic tenet of cognitive psychology is that perception of risk and hence perception of fear is driven by imaginability, not by data."}, {"time": 1738, "text": "And so we misallocate vast amounts of resources to avoiding terrorism, which kills on average about six Americans a year with one exception of 9 11."}, {"time": 1751, "text": "We invade countries, we invent entire new departments of government with massive, massive expenditure of resources and lives to defend ourselves against a trivial risk."}, {"time": 1765, "text": "Whereas guaranteed risks, one of them you mentioned traffic fatalities and even risks that are not here, but are plausible enough to worry about like pandemics, like nuclear war, receive far too little attention."}, {"time": 1785, "text": "In presidential debates, there's no discussion of how to minimize the risk of nuclear war."}, {"time": 1791, "text": "Lots of discussion of terrorism, for example."}, {"time": 1791, "text": "And so I think it's essential to calibrate our budget of fear, worry, concern, planning to the actual probability of harm."}, {"time": 1808, "text": "So let me ask this question."}, {"time": 1808, "text": "So speaking of imaginability, you said it's important to think about reason and one of my favorite people who likes to dip into the outskirts of reason through fascinating exploration of his imagination is Joe Rogan."}, {"time": 1823, "text": "So who has through reason used to believe a lot of conspiracies and through reason has stripped away a lot of his beliefs in that way."}, {"time": 1837, "text": "So it's fascinating actually to watch him through rationality kind of throw away the ideas of Bigfoot and 9 11."}, {"time": 1843, "text": "I'm not sure exactly."}, {"time": 1843, "text": "Kim Trails."}, {"time": 1843, "text": "I don't know what he believes in."}, {"time": 1850, "text": "But he no longer believed in."}, {"time": 1850, "text": "No, that's right."}, {"time": 1850, "text": "No, he's become a real force for good."}, {"time": 1855, "text": "So you were on the Joe Rogan podcast in February and had a fascinating conversation, but as far as I remember, didn't talk much about artificial intelligence."}, {"time": 1860, "text": "I will be on his podcast in a couple of weeks."}, {"time": 1865, "text": "Joe is very much concerned about existential threat of AI."}, {"time": 1865, "text": "I'm not sure if you're, this is why I was hoping that you would get into that topic."}, {"time": 1871, "text": "And in this way, he represents quite a lot of people who look at the topic of AI from 10,000 foot level."}, {"time": 1882, "text": "So as an exercise of communication, you said it's important to be rational and reason about these things."}, {"time": 1889, "text": "Let me ask, if you were to coach me as an AI researcher about how to speak to Joe and the general public about AI, what would you advise?"}, {"time": 1924, "text": "And they all plummet because the culture of engineering is how do you squeeze out the lethal risks, death by fire, death by drowning, death by asphyxiation, all of them drastically declined because of advances in engineering that I got to say, I did not appreciate until I saw those graphs."}]}]